# Topic-Toxicity Analysis in Online News Comments

A comprehensive analysis investigating which topics attract the most toxicity in online news comments, and whether article context influences toxicity perception.

## ğŸ“‹ Table of Contents
- [Overview](#overview)
- [Research Questions](#research-questions)
- [Dataset](#dataset)
- [Methodology](#methodology)
- [Results](#results)

## ğŸ” Overview

This project analyzes the **SFU Opinion and Comments Corpus (SOCC)**, containing 663,173 comments from The Globe and Mail (2012-2016), to identify which discussion topics attract the most toxic discourse. We employ multiple topic modeling techniques and investigate whether the same topics receive different toxicity ratings depending on the article context.

**Key Contributions:**
- Systematic ranking of topics by toxicity using 6,043 annotated comments
- Investigation of context-dependency in toxicity perception
- Comparative evaluation of 4 topic modeling methods (LDA, ProdLDA, BERTopic, NMF)
- Actionable insights for content moderation strategies

## â“ Research Questions

**RQ1:** Which comment topics attract the most toxicity in online news discussions?

**RQ2:** Does the article context (article topic/framing) modulate the perceived toxicity of comment topics?

**RQ3:** How do different topic modeling methods compare for toxicity-topic analysis?

## ğŸ“Š Dataset

### SFU Opinion and Comments Corpus (SOCC)

**Source:** [SOCC GitHub Repository](https://github.com/sfu-discourse-lab/SOCC)

**Contents:**
- 10,339 opinion articles from The Globe and Mail
- 663,173 unique comments
- 303,665 comment threads
- Time period: January 2012 - December 2016

**Annotated Subset:**
- 1,043 comments with expert toxicity labels (original)
- 5,000 additional comments annotated using pretrained models (our work)
- **Total:** 6,043 annotated comments

**Toxicity Scale:**
1. Not toxic
2. Mildly toxic
3. Toxic
4. Very toxic

## ğŸ”¬ Methodology

### Pipeline Overview
```
Phase 1: Toxicity Annotation
    â”œâ”€â”€ Select pretrained model (Perspective API / toxic-BERT)
    â”œâ”€â”€ Validate on 1,043 expert-labeled comments
    â””â”€â”€ Annotate 5,000 additional comments

Phase 2: Data Preparation
    â”œâ”€â”€ Stratified sampling (5,000 comments)
    â”œâ”€â”€ Quality control validation
    â””â”€â”€ Merge datasets (total: 6,043 comments)

Phase 3: Preprocessing
    â”œâ”€â”€ Text cleaning and normalization
    â”œâ”€â”€ Link comments to article metadata
    â””â”€â”€ Prepare for topic modeling

Phase 4: Topic Modeling (4 Methods)
    â”œâ”€â”€ LDA (Latent Dirichlet Allocation)
    â”œâ”€â”€ ProdLDA (Product of Experts LDA)
    â”œâ”€â”€ BERTopic (Transformer-based)
    â””â”€â”€ NMF (Non-Negative Matrix Factorization)

Phase 5: Topic Evaluation
    â”œâ”€â”€ Coherence scores (c_v, c_npmi, u_mass)
    â”œâ”€â”€ Topic diversity metrics
    â””â”€â”€ Model selection

Phase 6: Toxicity-Topic Analysis
    â”œâ”€â”€ Statistical testing (ANOVA/Kruskal-Wallis)
    â”œâ”€â”€ Effect size calculations
    â””â”€â”€ Control variable analysis

Phase 7: Context-Dependency Analysis
    â”œâ”€â”€ Article topic categorization
    â”œâ”€â”€ Duplicate comment analysis
    â”œâ”€â”€ Topic-context interaction modeling
    â””â”€â”€ Context effect quantification

Phase 8: Visualization & Interpretation
    â”œâ”€â”€ Topic-toxicity heatmaps
    â”œâ”€â”€ Context interaction visualizations
    â””â”€â”€ Temporal trend analysis
```

## ğŸ“ˆ Results
